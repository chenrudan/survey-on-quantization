# survey-on-quantization
A survey of Quantization papers and codes


[ICLR2022] F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization [https://openreview.net/forum?id=_CfpJazzXT2] [https://github.com/snap-research/F8Net]

[NeurIPS2021] MQBench: Towards Reproducible and Deployable Model Quantization Benchmark [https://arxiv.org/pdf/2111.03759.pdf] [http://mqbench.tech/]

[CVPR2021] EWGS:Network Quantization with Element-wise Gradient Scaling [https://arxiv.org/abs/2104.00903] [https://github.com/cvlab-yonsei/EWGS]

[ICLR2021] BRECQ: PUSHING THE LIMIT OF POST-TRAINING QUANTIZATION BY BLOCK RECONSTRUCTION [https://arxiv.org/pdf/2102.05426.pdf] [https://github.com/yhhhli/BRECQ]

[AAAI2021] Distribution Adaptive INT8 Quantization for Training CNNs [https://arxiv.org/pdf/2102.04782.pdf] [None]

[ICLR2020] LSQï¼šLearned Step Size Quantization [https://arxiv.org/abs/1902.08153] [https://github.com/zhutmost/lsq-net]

[ICLR2020] APoT: Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks [https://arxiv.org/abs/1909.13144] [https://github.com/yhhhli/APoT_Quantization]

[CVPR202] Towards Unified INT8 Training for Convolutional Neural Network [https://arxiv.org/abs/1912.12607] [None]

[ICLR2018] PACT: Parameterized Clipping Activation for Quantized Neural Networks [https://openreview.net/pdf?id=By5ugjyCb] [https://github.com/KwangHoonAn/PACT]

